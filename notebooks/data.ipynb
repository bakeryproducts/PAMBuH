{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('..')\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from functools import partial, reduce\n",
    "from collections import defaultdict\n",
    "import multiprocessing as mp\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import utils\n",
    "from sampler import GdalSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narezator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def poolcontext(*args, **kwargs):\n",
    "    pool = mp.Pool(*args, **kwargs)\n",
    "    yield pool\n",
    "    pool.terminate()\n",
    "    \n",
    "def mp_func(foo, args, n):\n",
    "    args_chunks = [args[i:i + n] for i in range(0, len(args), n)]\n",
    "    with poolcontext(processes=n) as pool:\n",
    "        pool.map(foo, args_chunks)\n",
    "    \n",
    "def mp_foo(foo, args): return foo(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_sampler(dst, i_fn, a_fn, wh, wh_mask, idxs):\n",
    "    s = GdalSampler(i_fn, a_fn, wh, wh_mask)\n",
    "    for idx in idxs:\n",
    "        i,m = s[idx]  \n",
    "        \n",
    "        img_dir = dst / 'imgs' / i_fn.with_suffix('').name\n",
    "        os.makedirs(str(img_dir), exist_ok=True)\n",
    "        \n",
    "        mask_dir = dst / 'masks' / i_fn.with_suffix('').name\n",
    "        os.makedirs(str(mask_dir), exist_ok=True)\n",
    "        \n",
    "        orig_name = (str(idx) + '.png')\n",
    "        img_name = img_dir / orig_name \n",
    "        mask_name = mask_dir /orig_name\n",
    "        \n",
    "        cv2.imwrite(str(img_name), i.transpose(1,2,0))\n",
    "        cv2.imwrite(str(mask_name), np.expand_dims(m,-1).repeat(3,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('input/hm/train')\n",
    "#p = Path('/home/sokolov/work/webinf/data/kidney/train/')\n",
    "dst_path = Path('input/train')\n",
    "NUM_PROC = 16\n",
    "wh = (2048, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = partial(utils.filter_ban_str_in_name, bans=['-', '_ell'])\n",
    "ann_fns = utils.get_filenames(p, '*.json', filt)\n",
    "img_fns = [a.with_suffix('.tiff') for a in ann_fns]\n",
    "img_fns[0], ann_fns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert  False , 'DO ONCE'\n",
    "# for i_fn,a_fn in tqdm(zip(img_fns, ann_fns)):\n",
    "#     const_args = i_fn, a_fn, wh, wh\n",
    "#     _s = GdalSampler(*const_args)\n",
    "#     part_samp = partial(mp_sampler, *(dst_path, *const_args))\n",
    "#     mp_func(part_samp, range(len(_s)), NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, root, pattern):\n",
    "        self.root = Path(root)\n",
    "        self.pattern = pattern\n",
    "        self.files = sorted(list(self.root.glob(self.pattern)))\n",
    "        self._is_empty('There is no matching files!')\n",
    "        \n",
    "    def apply_filter(self, filter_fn):\n",
    "        self.files = filter_fn(self.files)\n",
    "        self._is_empty()\n",
    "\n",
    "    def _is_empty(self, msg='There is no item in dataset!'): assert len(self.files) > 0\n",
    "    def __len__(self): return len(self.files)\n",
    "    def __getitem__(self, idx): return self.process_item(self.load_item(idx))\n",
    "    def load_item(self, idx): raise NotImplementedError\n",
    "    def process_item(self, item): return item\n",
    "    \n",
    "class ImageDataset(Dataset):\n",
    "    def load_item(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        img = Image.open(str(img_path))\n",
    "        return img\n",
    "    \n",
    "class PairDataset:\n",
    "    def __init__(self, ds1, ds2):\n",
    "        self.ds1, self.ds2 = ds1, ds2\n",
    "        self.check_len()\n",
    "    \n",
    "    def __len__(self): return len(self.ds1)\n",
    "    def check_len(self): assert len(self.ds1) == len(self.ds2)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.ds1.__getitem__(idx), self.ds2.__getitem__(idx) \n",
    "\n",
    "class ConcatDataset:\n",
    "    \"\"\"\n",
    "    To avoid recursive calls (like in torchvision variant)\n",
    "    \"\"\"\n",
    "    def __init__(self, dss):\n",
    "        self.length = 0\n",
    "        self.ds_map = {}\n",
    "        for i, ds in enumerate(dss):\n",
    "            for j in range(len(ds)):\n",
    "                self.ds_map[j+self.length] = i, self.length\n",
    "            self.length += len(ds)\n",
    "        self.dss = dss\n",
    "    \n",
    "    def load_item(self, idx):\n",
    "        if idx >= self.__len__(): raise StopIteration\n",
    "        ds_idx, local_idx = self.ds_map[idx]\n",
    "        return self.dss[ds_idx].__getitem__(idx - local_idx)\n",
    "    \n",
    "    def _is_empty(self, msg='There is no item in dataset!'): assert len(self.files) > 0\n",
    "    def __len__(self): return self.length\n",
    "    def __getitem__(self, idx): return self.load_item(idx)\n",
    "\n",
    "def expander(x):\n",
    "    x = np.array(x)\n",
    "    return x if len(x.shape) == 3 else np.repeat(np.expand_dims(x, axis=-1), 3, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pam specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentDataset:\n",
    "    def __init__(self, imgs_path, masks_path, mode_train=True):\n",
    "        self.img_folders = utils.get_filenames(imgs_path, '*', lambda x: False)\n",
    "        self.masks_folders = utils.get_filenames(masks_path, '*', lambda x: False)\n",
    "        self.mode_train = mode_train\n",
    "        \n",
    "        dss = []\n",
    "        for imgf, maskf in zip(self.img_folders, self.masks_folders):\n",
    "            ids = ImageDataset(imgf, '*.png')\n",
    "            mds = ImageDataset(maskf, '*.png')\n",
    "            if self.mode_train:\n",
    "                ids.process_item = expander\n",
    "                mds.process_item = expander\n",
    "            dss.append(PairDataset(ids, mds))\n",
    "        \n",
    "        self.dataset = ConcatDataset(dss)\n",
    "    \n",
    "    def __len__(self): return len(self.dataset)\n",
    "    def __getitem__(self, idx): return self.dataset[idx]\n",
    "    def _view(self, idx):\n",
    "        pair = self.__getitem__(idx)\n",
    "        return Image.blend(*pair,.5)\n",
    "    \n",
    "def build_datasets(mode_train=True):\n",
    "    root = Path('input/train/1024')\n",
    "    sd = SegmentDataset(root / 'imgs', root / 'masks', mode_train=mode_train)\n",
    "    return {'TRAIN':sd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('input/train/1024')\n",
    "sd = SegmentDataset(root / 'imgs', root / 'masks', mode_train=False)\n",
    "len(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "@interact(idx=(0, len(sd)),continuous_update=False)\n",
    "def view(idx): return sd._view(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset, sampler, shuffle, batch_size, num_workers, drop_last, pin):\n",
    "    dl = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin,\n",
    "        drop_last=drop_last,\n",
    "        collate_fn=None,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloaders(datasets, samplers=None, batch_sizes=None, num_workers=1, drop_last=False, pin=False):\n",
    "    dls = {}\n",
    "    for kind, dataset in datasets.items():\n",
    "        sampler = samplers[kind]    \n",
    "        shuffle = kind == 'TRAIN' if sampler is None else False\n",
    "        batch_size = batch_sizes[kind] if batch_sizes[kind] is not None else 1\n",
    "        dls[kind] = create_dataloader(dataset, sampler, shuffle, batch_size, num_workers, drop_last, pin)\n",
    "            \n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = build_datasets()\n",
    "dls = build_dataloaders(datasets,samplers={'TRAIN':None}, batch_sizes={'TRAIN':32}, num_workers=4, pin=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdl = dls['TRAIN']\n",
    "for b in tdl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0].shape, b[0].dtype, b[1].shape, b[1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
